{% disable_client_cache %}

<style>
        pre {
            background-color: #f4f4f4;
            padding: 10px;
            border-radius: 5px;
            overflow-x: auto;
        }
        code {
            color: #d63384;
            font-family: Consolas, Monaco, "Andale Mono", "Ubuntu Mono", monospace;
        }
    </style>

<h1>APIs Usage Examples</h1>

<h2>Text Generation (Mistral, Granite,...)</h2>
<h3>Using Curl</h3>
<pre>
<code>
curl -X 'POST' \
    'https://granite-33-2b-instruct-maas-apicast-production.apps.cluster-7tqcw.7tqcw.sandbox3194.opentlc.com:443/v1/completions' \
    -H 'accept: application/json' \
    -H 'Content-Type: application/json' \
    -H 'Authorization: Bearer ***************************' \
    -d '{
    "model": "granite-33-2b-instruct",
    "prompt": "San Francisco is a",
    "max_tokens": 15,
    "temperature": 0
}'
</code>
</pre>

<h3>Using raw Python</h3>
<pre>
<code>
import requests
import urllib3
import numpy as np
import json

API_URL = "https://granite-33-2b-instruct-maas-apicast-production.apps.cluster-7tqcw.7tqcw.sandbox3194.opentlc.com:443"
API_KEY = "***************************"

input = ["San Francisco is a"]

completion = requests.post(
    url=API_URL+'/v1/completions',
    json={
      "model": "granite-33-2b-instruct",
      "prompt": "San Francisco is a",
      "max_tokens": 15,
      "temperature": 0
    },
    headers={'Authorization': 'Bearer '+API_KEY}
).json()

print(completion)
</code>
</pre>

<h3>Using Langchain</h3>
Prerequisites: <code>pip install langchain-community</code>
<pre>
<code>
from langchain_community.llms import VLLMOpenAI

API_URL = "https://granite-33-2b-instruct-maas-apicast-production.apps.cluster-7tqcw.7tqcw.sandbox3194.opentlc.com:443"
API_KEY = "***************************"

llm = VLLMOpenAI(
    openai_api_key=API_KEY,
    openai_api_base=API_URL+"/v1",
    model_name="granite-33-2b-instruct",
    model_kwargs={"stop": ["."]},
)
print(llm.invoke("Rome is"))
</code>
</pre>

<h3>Connecting Continue.dev to Granite-Code-Instruct</h3>
Configuration in <code>.continue/config.json</code>
<pre>
<code>
...
  "models": [
    {
      "title": "Granite-2B-Instruct",
      "provider": "openai",
      "model": "granite-33-2b-instruct",
      "apiBase": "https://granite-33-2b-instruct-maas-apicast-production.apps.cluster-7tqcw.7tqcw.sandbox3194.opentlc.com:443/v1/",
      "apiKey": "************************",
      "completionOptions": {
        "temperature": 0.1,
        "topK": 1,
        "topP": 1,
        "presencePenalty": 0,
        "frequencyPenalty": 0
      }
    }
  ]
...
  "tabAutocompleteModel": {
    "title": "Granite-2B-Instruct",
    "provider": "openai",
    "model": "granite-33-2b-instruct",
    "apiBase": "https://granite-33-2b-instruct-maas-apicast-production.apps.cluster-7tqcw.7tqcw.sandbox3194.opentlc.com:443/v1/",
    "apiKey": "****************************",
    "completionOptions": {
      "temperature": 0.1,
      "topK": 1,
      "topP": 1,
      "presencePenalty": 0,
      "frequencyPenalty": 0
    }
  },
  "tabAutocompleteOptions": {
    "useCopyBuffer": false,
    "maxPromptTokens": 1024,
    "prefixPercentage": 0.5
  },
...
</code>
</pre>